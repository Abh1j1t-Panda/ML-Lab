# -*- coding: utf-8 -*-
"""Copy of ML lab(complete)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zlt1KqNfEOWnQ4UDDBvAyRfQRTASqimZ

**Exp 1 features**
"""

from google.colab import drive
drive.mount('/content/drive')
import csv
import numpy as np
import pandas as pd
mydata=pd.read_csv('/content/Iris.csv')
mydata.tail()

x=pd.DataFrame(mydata)
import matplotlib.pyplot as plt
x.plot()

x['PetalWidthCm'].plot(kind='hist')

plt.scatter(x['PetalWidthCm'],x['SepalWidthCm'])

X=x['PetalLengthCm']
y=x['SepalLengthCm']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

X_train

X_test

y_train

y_test

from scipy import stats

x1 = x['PetalLengthCm']
y = x['SepalLengthCm']

slope, intercept, r, p, std_err = stats.linregress(x1,y)

def myfunc(x):
  return slope * x + intercept

mymodel = list(map(myfunc, x1))

plt.scatter(x1, y,edgecolors='yellow')
plt.plot(x1, mymodel,'red')
plt.show()

"""**Linear Regression**"""

from google.colab import drive
drive.mount('/content/drive')

import csv
import numpy as np
import pandas as pd
mydata=pd.read_csv('/content/Data_final.csv')
mydata.tail()

x=pd.DataFrame(mydata)

# Commented out IPython magic to ensure Python compatibility.

import matplotlib.pyplot as plt

# %matplotlib inline

import seaborn as sns

mydata.info()

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
label_encoder_x= LabelEncoder()
mydata['Career']= label_encoder_x.fit_transform(mydata['Career'])

mydata['Career']

sns.pairplot(mydata)

x=mydata[['Numerical Aptitude']]
y=mydata[['Verbal Reasoning']]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

import numpy as np
from sklearn.linear_model import LinearRegression

model = LinearRegression()

model.fit(X_train, y_train)

print(model.coef_)

print(model.intercept_)

pd.DataFrame(model.coef_, x.columns, columns = ['Coeff'])

predictions = model.predict(X_test)

plt.scatter(y_test, X_test, color ='r')
plt.scatter(y_test, predictions, color ='b')

plt.hist(y_test - predictions)

y_test

from sklearn import metrics

print(metrics.mean_absolute_error(y_test, predictions))

print(metrics.mean_squared_error(y_test, predictions))

print(np.sqrt(metrics.mean_squared_error(y_test, predictions)))

from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train,y_train)
r2_score = regressor.score(X_test,y_test)
print(r2_score*100,'%')

"""**Decision Tree**"""

from google.colab import drive
drive.mount('/content/drive')
import csv
import numpy as np
import pandas as pd
mydata=pd.read_csv('/content/Data_final.csv')
mydata.head()

import seaborn as sns
sns.pairplot(mydata)

mydata.tail()

x=pd.DataFrame(mydata)

features=['A_score','E_score']

X=x[features]
Y=x['Career']
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
dtree=DecisionTreeClassifier()
dtree=dtree.fit(X,Y)
tree.plot_tree(dtree,feature_names=features)

"""**Data Preprocessing**"""

mydata['Career'].unique()

mydata.describe(include='all')

mydata.drop(columns="O_score",inplace=True)

mydata.head(2)

"""**Data Visualization**"""

import seaborn as sns
import matplotlib.pyplot as plt
sns.pairplot(mydata)

g=sns.relplot(x='C_score',y='N_score',data=mydata,hue='Career',style='Career')
g.fig.set_size_inches(10,5)
plt.show

"""**Dividing Data into Features and Lables**"""

x=mydata.iloc[:,0:4].values
y=mydata.iloc[:,4].values

x

y

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
y=le.fit_transform(y)
y

"""**Model Building**"""

from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score

from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test=train_test_split(x,y,test_size=0.3,random_state=42)

"""**Gaussian Naive Bayes**"""

from sklearn.naive_bayes import GaussianNB
gaussian= GaussianNB()
gaussian.fit(X_train,Y_train)
Y_pred=gaussian.predict(X_test)
accuracy_nb=round(accuracy_score(Y_test,Y_pred)*100,2)
acc_gaussian=round(gaussian.score(X_train,Y_train)*100,2)
cm=confusion_matrix(Y_test,Y_pred)
accuracy=accuracy_score(Y_test,Y_pred)
precision=precision_score(Y_test,Y_pred,average='micro')
recall=recall_score(Y_test,Y_pred,average='micro')
f1=f1_score(Y_test,Y_pred,average='micro')
print("confusion matrux for naive bayes \n",cm)
print('accuracy_Naive bayes:%3f',accuracy)
print('recall-Naive bayes :%3f',recall)
print('f1-score Naive bayes :%3f',f1)

"""**Support Vector Machine**"""

from sklearn.svm import SVC, LinearSVC

linear_SVC = LinearSVC(max_iter=4000)
linear_SVC.fit(X_train, Y_train)
Y_pred = linear_SVC.predict(X_test)
accuracy_svc = round(accuracy_score(Y_test, Y_pred) * 100, 2)
acc_linear_SVC = round(linear_SVC.score(X_train, Y_train) * 100, 2)
cm=confusion_matrix(Y_test,Y_pred)
accuracy= accuracy_score(Y_test,Y_pred)
precision=precision_score(Y_test,Y_pred,average='micro')
recall=recall_score(Y_test,Y_pred,average='micro')
f1=f1_score(Y_test,Y_pred,average='micro')
print("confusion matrux for SVM \n",cm)
print('accuracy_SvM:%3f',accuracy)
print('recall-SVM :%3f',recall)
print('f1-score SvM :%3f',f1)

"""**Random forest Classifier**"""

from google.colab import drive
drive.mount('/content/drive')
import csv
import numpy as np
import pandas as pd
mydata=pd.read_csv('/content/depression_sampling - Sheet1.csv')
mydata.head(2)

mydata['depression'].unique()

mydata.describe(include='all')

mydata.drop(columns='nutral',inplace=True)

mydata.head(2)

import seaborn as sns
import matplotlib.pyplot as plt
sns.pairplot(mydata)

g=sns.relplot(x='happy',y='fear',data=mydata,hue='depression',style='depression')
g.fig.set_size_inches(10,5)
plt.show()

X=mydata.iloc[:,0:4].values
y=mydata.iloc[:,4].values
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score ,precision_score,recall_score,f1_score
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# prompt: implement random forest classifier

from sklearn.ensemble import RandomForestClassifier

random_forest = RandomForestClassifier(n_estimators=100)
random_forest.fit(X_train, y_train)
Y_prediction = random_forest.predict(X_test)

from sklearn.metrics import confusion_matrix, accuracy_score

cm = confusion_matrix(y_test, Y_prediction)
print('confusion matrix of classifier \n',cm)
accuracy = accuracy_score(y_test, Y_prediction)
print('accruacry of randpom forest classifier',accuracy)

"""**K means Clustering**"""

from google.colab import drive
drive.mount('/content/drive')
import csv
import pandas as pd
import numpy as np
mydata=pd.read_csv('/content/Iris.csv')
mydata.head(2)

mydata['Species'].unique()

mydata.describe(include='all')

import seaborn as sns
import matplotlib.pyplot as plt

# Plotting distribution of 'condition' by 'model'
sns.FacetGrid(mydata, hue="Species", height=3).map(sns.distplot, "SepalLengthCm").add_legend()

# Plotting distribution of 'odometer' by 'model'
sns.FacetGrid(mydata, hue="Species", height=3).map(sns.distplot, "PetalLengthCm").add_legend()

# Plotting distribution of 'year' by 'model'
sns.FacetGrid(mydata, hue="Species", height=3).map(sns.distplot, "SepalWidthCm").add_legend()

# Plotting distribution of 'sellingprice' by 'model'
sns.FacetGrid(mydata, hue="Species", height=3).map(sns.distplot, "PetalLengthCm").add_legend()

plt.show()

import seaborn as sns
import matplotlib.pyplot as plt


sns.set_style("whitegrid")
sns.pairplot(mydata,hue="Species",size=3);
plt.show()

mydata.drop(columns="Id",inplace=True)

df = pd.read_csv("/content/Iris.csv")
x = df.iloc[:, [0,1, 2, 3]].values

x

mydata.head(2)

df=mydata.replace([np.nan,-np.inf],0)

from sklearn.cluster import KMeans

wcss = []

for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
    kmeans.fit(x)
    wcss.append(kmeans.inertia_)

plt.plot(range(1, 11), wcss)
plt.title('The elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS') #within cluster sum of squares
plt.show()

kmeans = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)
y_kmeans = kmeans.fit_predict(x)

plt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], s = 100, c = 'purple', label = 'Iris-setosa')
plt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], s = 100, c = 'orange', label = 'Iris-versicolour')
plt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Iris-virginica')

#Plotting the centroids of the clusters
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 100, c = 'red', label = 'Centroids')

plt.legend()

fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(111, projection='3d')
plt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], s = 100, c = 'purple', label = 'Iris-setosa')
plt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], s = 100, c = 'orange', label = 'Iris-versicolour')
plt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Iris-virginica')

#Plotting the centroids of the clusters
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 100, c = 'red', label = 'Centroids')
plt.show()